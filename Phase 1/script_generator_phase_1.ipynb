{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yrwEbuCZ86KH"
   },
   "source": [
    "# Setup the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qHQdvK2T73fI"
   },
   "outputs": [],
   "source": [
    "''' Edit these variables to pick what course you want generated  '''\n",
    "source_language = \"English\"\n",
    "target_language = \"کوردی سۆرانی\"\n",
    "CEFR_level = \"A1\"\n",
    "topic_of_interest = \"Tourism\"\n",
    "number_of_topics_to_generate = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "'''  Pick your model and API key over here  '''\n",
    "models = {\n",
    "    'deepseek': {\n",
    "        'name': 'deepseek/deepseek-r1:free',\n",
    "        'structured_outputs': False,\n",
    "        'temperature': 0\n",
    "    },\n",
    "    'claude 3.7': {\n",
    "        'name': 'anthropic/claude-3.7-sonnet',\n",
    "        'structured_outputs': False,\n",
    "        'temperature': 0\n",
    "    },\n",
    "    'gemini 2.0': {\n",
    "        'name': 'google/gemini-2.0-flash-exp:free',\n",
    "        'structured_outputs': True,\n",
    "        'temperature': 0\n",
    "    },\n",
    "    'gemini 2.5 exp': {\n",
    "        'name': 'google/gemini-2.5-pro-exp-03-25:free',\n",
    "        'structured_outputs': True,\n",
    "        'temperature': 0\n",
    "    },\n",
    "    'gemini 2.5 preview': {\n",
    "        'name': 'google/gemini-2.5-pro-preview-03-25',\n",
    "        'structured_outputs': True,\n",
    "        'temperature': 0\n",
    "    },\n",
    "}\n",
    "default_model = models['gemini 2.5 preview']\n",
    "api_key = \"\"\n",
    "with open(\".env\", \"r\") as f:\n",
    "    api_key = f.read().strip().split(\"=\")[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ony4dK4ZF1wm"
   },
   "source": [
    "# Program Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the output folder where the course outline and scripts will be stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ikeStO478Q6J"
   },
   "outputs": [],
   "source": [
    "output_folder = f\"Courses/{source_language}-to-{target_language} at {CEFR_level}-level Lesson Scripts ({topic_of_interest})/\"\n",
    "os.makedirs(os.path.dirname(output_folder), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some functions that will be used throughout the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MLBrIMNjF_Fy"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "'''\n",
    "Function to turn a response_format object into a text-based template of the desired response\n",
    "\n",
    "(this is used for models that don't have strcutured output functions and need the JSON schema to be specified in the prompt)\n",
    "'''\n",
    "def schema_to_template(schema_object: dict) -> str:\n",
    "    def process_schema_type(schema):\n",
    "        \"\"\"\n",
    "        Recursively process different schema types to create template values\n",
    "        \"\"\"\n",
    "        if schema.get(\"type\") == \"object\":\n",
    "            # Handle object type\n",
    "            obj_template = {}\n",
    "            if \"properties\" in schema:\n",
    "                for prop_name, prop_details in schema[\"properties\"].items():\n",
    "                    # Include description if available\n",
    "                    description = prop_details.get(\"description\", \"\")\n",
    "\n",
    "                    # Recursively process nested types\n",
    "                    if prop_details.get(\"type\") == \"object\":\n",
    "                        obj_template[prop_name] = process_schema_type(prop_details)\n",
    "                    elif prop_details.get(\"type\") == \"array\":\n",
    "                        # Handle array types\n",
    "                        if \"items\" in prop_details:\n",
    "                            items_schema = prop_details[\"items\"]\n",
    "                            if items_schema.get(\"type\") == \"string\":\n",
    "                                obj_template[prop_name] = [f\"string ({description})\"]\n",
    "                            elif items_schema.get(\"type\") == \"object\":\n",
    "                                obj_template[prop_name] = [process_schema_type(items_schema)]\n",
    "                            else:\n",
    "                                obj_template[prop_name] = [f\"{items_schema.get('type', 'unknown')} ({description})\"]\n",
    "                        else:\n",
    "                            obj_template[prop_name] = []\n",
    "                    elif prop_details.get(\"type\") == \"string\":\n",
    "                        obj_template[prop_name] = f\"string ({description})\"\n",
    "                    elif prop_details.get(\"type\") == \"boolean\":\n",
    "                        obj_template[prop_name] = \"boolean\"\n",
    "                    elif prop_details.get(\"type\") == \"number\":\n",
    "                        obj_template[prop_name] = \"number\"\n",
    "            return obj_template\n",
    "        elif schema.get(\"type\") == \"array\":\n",
    "            # Handle array type\n",
    "            if \"items\" in schema:\n",
    "                items_schema = schema[\"items\"]\n",
    "                if items_schema.get(\"type\") == \"object\":\n",
    "                    return [process_schema_type(items_schema)]\n",
    "                elif items_schema.get(\"type\") == \"string\":\n",
    "                    description = items_schema.get(\"description\", \"\")\n",
    "                    return [f\"string ({description})\"]\n",
    "            return []\n",
    "\n",
    "        return schema.get(\"type\", \"unknown\")\n",
    "\n",
    "    # Extract the schema from the input object\n",
    "    schema = schema_object.get(\"json_schema\", {}).get(\"schema\", schema_object)\n",
    "\n",
    "    # Process the schema\n",
    "    template = process_schema_type(schema)\n",
    "\n",
    "    # Convert to JSON string with indentation\n",
    "    return f'\\nThe JSON should be structured like this:\\n{json.dumps(template, indent=2)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9uz4ScSXGBzr"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "'''\n",
    "Function that sends the given prompt to the specified LLM in the desired format.\n",
    "Returns the text-only response of the LLM if there are no errors, returns None otherwise\n",
    "'''\n",
    "def call_LLM(prompt: str, system_prompt:str=\"\", response_format: dict = {\"type\":\"text\"}, model_to_use:dict = default_model) -> str:\n",
    "  messages = []\n",
    "  if system_prompt != \"\":\n",
    "    messages.append({\n",
    "      \"role\": \"system\",\n",
    "      \"content\": system_prompt\n",
    "    })\n",
    "  messages.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": prompt + ('' if (model_to_use['structured_outputs'] and response_format[\"type\"] == \"json_schema\")  else schema_to_template(response_format))\n",
    "  })\n",
    "      \n",
    "  response = requests.post(\n",
    "    url=\"https://openrouter.ai/api/v1/chat/completions\",\n",
    "    headers={\n",
    "      \"Authorization\": f\"Bearer {api_key}\",\n",
    "      \"Content-Type\": \"application/json\",\n",
    "    },\n",
    "    data=json.dumps({\n",
    "      \"model\": model_to_use['name'],\n",
    "      \"temperature\": model_to_use['temperature'],\n",
    "      \"structured_outputs\": model_to_use['structured_outputs'] and response_format[\"type\"] != \"text\",\n",
    "      \"messages\": messages,\n",
    "      \"response_format\": response_format\n",
    "    })\n",
    "  )\n",
    "\n",
    "  if 'error' in response.json():\n",
    "    print(response.json()['error']['message'])\n",
    "    return None\n",
    "\n",
    "  return response.json()['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dAosZO_mGD4i"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Function that parses a text-based response from an LLM and converts it into a valid JSON object\n",
    "'''\n",
    "def response_to_JSON(text_response: str):\n",
    "  start_token = ''\n",
    "  end_token = ''\n",
    "  for char in text_response:\n",
    "    if char == '{':\n",
    "      start_token = '{'\n",
    "      end_token = '}'\n",
    "    elif char == '[':\n",
    "      start_token = '['\n",
    "      end_token = ']'\n",
    "\n",
    "    if start_token != '':\n",
    "      break\n",
    "\n",
    "  trimmed_response = text_response[text_response.index(start_token):text_response.rindex(end_token)+1]\n",
    "  return json.loads(trimmed_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to load the desired prompt in the desired format from their two corresponding files\n",
    "'''\n",
    "def load_prompt_and_format(path_to_prompt:str, path_to_format:str):\n",
    "    prompt = \"\"\n",
    "    with open(path_to_prompt, 'r', encoding='utf-8') as file:\n",
    "        prompt = file.read()\n",
    "        \n",
    "    response_format = {\n",
    "        \"type\": \"json_schema\",\n",
    "        \"json_schema\": {\n",
    "            \"name\": \"response\",\n",
    "            \"strict\": True,\n",
    "        }\n",
    "    }\n",
    "    with open(path_to_format, 'r') as file:\n",
    "        format = json.loads(file.read())\n",
    "        response_format['json_schema']['schema'] = format\n",
    "    \n",
    "    return prompt, response_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function that loops through all the specified tags in a given prompt and replaces them with the appropriate replacements\n",
    "'''\n",
    "def replace_tags(prompt:str, replacements:dict):  \n",
    "    for tag, replacement in replacements.items():\n",
    "        prompt = prompt.replace(tag, str(replacement))\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to handle errors in calling the LLM and converting the response to JSON by continuously trying again until it succeeds\n",
    "'''\n",
    "def continuously_call_LLM(prompt:str, system_prompt:str = \"\", response_format:dict = {'type': 'text'}, model_to_use:dict = default_model, max_tries:int = 5):\n",
    "    result = None\n",
    "    \n",
    "    number_of_fails = 0\n",
    "    while result == None:\n",
    "        # Handling errors with calling the LLM\n",
    "        try:\n",
    "            result = call_LLM(prompt, system_prompt, response_format, model_to_use)\n",
    "        except Exception as e:\n",
    "            print(f'Calling the LLM faield. {e}. Trying again...')\n",
    "        \n",
    "        # Handling errors with converting the LLM's response to a JSON object\n",
    "        if response_format['type'] == 'json_schema' and result != None:\n",
    "            try:\n",
    "                result = response_to_JSON(result)\n",
    "            except Exception as e:\n",
    "                result = None\n",
    "                print(f'Converting response to JSON failed. {e}. Trying again...')\n",
    "        \n",
    "        # If the model failed too many times, leave the loop to not get stuck in an infinite cycle\n",
    "        number_of_fails += 1\n",
    "        if number_of_fails >= max_tries:\n",
    "            print(f'-ERROR, Max number of tries reached, exiting the loop-')\n",
    "            break\n",
    "    \n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to save a generated JSON object to a file in the output folder\n",
    "'''\n",
    "def saveJSON(obj: dict, information_name: str) -> None:\n",
    "  with open(f\"{output_folder}{information_name}.json\", \"w\") as f:\n",
    "    json.dump(obj, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to load a previously generated JSON object from a file in the output folder\n",
    "'''\n",
    "def loadJSON(information_name: str) -> None:\n",
    "  with open(f\"{output_folder}{information_name}.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    return json.loads(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axyKgI3F8nk8"
   },
   "source": [
    "# Generate the Grammar Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts_list = []\n",
    "file_name = \"Grammar Concepts\"\n",
    "load_existing_grammar_concepts = True\n",
    "\n",
    "if load_existing_grammar_concepts:\n",
    "    concepts_list = loadJSON(file_name)\n",
    "\n",
    "else:\n",
    "    prompt, response_format = load_prompt_and_format(\n",
    "        \"Prompts/grammar_generator_instructions.txt\",\n",
    "        \"Prompts/grammar_generator_format.json\"\n",
    "    )\n",
    "    replacements = {\n",
    "        \"[target_language]\": target_language,\n",
    "        \"[source_language]\": source_language,\n",
    "        \"[CEFR_level]\": CEFR_level,\n",
    "    }\n",
    "    \n",
    "    prompt = replace_tags(prompt, replacements)\n",
    "    concepts_list = continuously_call_LLM(prompt, response_format=response_format)\n",
    "    \n",
    "    saveJSON(concepts_list, file_name)\n",
    "\n",
    "concepts_as_text = \"\"\n",
    "for i, concept in enumerate(concepts_list):\n",
    "    print(f'{i+1}) {concept[\"name\"]}')\n",
    "    for component in concept[\"components\"]:\n",
    "        print(f'  - {component[\"description\"]} ({', '.join(component[\"examples\"])})')\n",
    "        concepts_as_text += f'{concept[\"name\"]} | {component[\"description\"]} ({', '.join(component[\"examples\"])})\\n'\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bpzHHYSl809i"
   },
   "source": [
    "# Generate the Course Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 30\n",
    "\n",
    "system_prompt = \"\"\n",
    "with open(\"Prompts/iterative_system.txt\", 'r', encoding='utf-8') as file:\n",
    "    system_prompt = file.read()\n",
    "    \n",
    "replacements = {\n",
    "    \"[target_language]\": target_language,\n",
    "    \"[source_language]\": source_language,\n",
    "    \"[CEFR_level]\": CEFR_level,\n",
    "    \"[topic_of_interest]\": topic_of_interest,\n",
    "}\n",
    "\n",
    "system_prompt = replace_tags(system_prompt, replacements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = []\n",
    "file_name = \"Topic Distribution\"\n",
    "load_existing_topics = True\n",
    "\n",
    "if load_existing_topics:\n",
    "    topics = loadJSON(file_name)\n",
    "\n",
    "else:\n",
    "    prompt, response_format = load_prompt_and_format(\n",
    "        \"Prompts/iterative_prompt_topics.txt\",\n",
    "        \"Prompts/iterative_format_topics.json\"\n",
    "    )\n",
    "    replacements = {\n",
    "        \"[target_language]\": target_language,\n",
    "        \"[CEFR_level]\": CEFR_level,\n",
    "        \"[topic_of_interest]\": topic_of_interest,\n",
    "        \"[num_topics]\": num_topics,\n",
    "    }\n",
    "    \n",
    "    prompt = replace_tags(prompt, replacements)\n",
    "    topics = continuously_call_LLM(prompt, system_prompt, response_format)\n",
    "    \n",
    "    saveJSON(topics, file_name)\n",
    "\n",
    "# Print the topic distribution\n",
    "for i, topic in enumerate(topics):\n",
    "    print(f'{i+1}) {topic[\"topic\"]}')\n",
    "    print('  New Phrases:')\n",
    "    for new_phrase in topic[\"new_phrases\"]:\n",
    "        print(f'    - {new_phrase}')\n",
    "    print('  New Skills:')\n",
    "    for new_skill in topic[\"new_skills\"]:\n",
    "        print(f'    - {new_skill}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4XNI3ASv9pbe"
   },
   "outputs": [],
   "source": [
    "class LearningItem:\n",
    "    def __init__(self, content: str, start_lesson: int, item_type: str):\n",
    "        self.content = content\n",
    "        self.start_lesson = start_lesson\n",
    "        self.item_type = item_type\n",
    "        self.exposure_count = 0\n",
    "        self.last_exposure_day = start_lesson\n",
    "        self.next_review_day = start_lesson+1\n",
    "        self.difficulty_level = 1\n",
    "        self.complexity_score = 1 if item_type != 'concept' else 3  # Default to 3 for concepts\n",
    "\n",
    "    def update_after_exposure(self, current_day):\n",
    "        self.exposure_count += 1\n",
    "        self.last_exposure_day = current_day\n",
    "        self.set_next_review_day(current_day)\n",
    "\n",
    "    def set_next_review_day(self, current_day):\n",
    "        intervals = [1, 3, 7, 14, 30, 60]\n",
    "\n",
    "        if self.exposure_count <= 5:\n",
    "            interval = [1, 2, 3, 5, 7][self.exposure_count - 1]\n",
    "        else:\n",
    "            interval = intervals[min(self.exposure_count - 1, len(intervals) - 1)]\n",
    "\n",
    "        if self.item_type == 'concept':\n",
    "            interval = int(interval * (self.complexity_score * 0.5))\n",
    "        else:\n",
    "            if self.difficulty_level == 2:\n",
    "                interval = int(interval * 1.5)\n",
    "            elif self.difficulty_level == 3:\n",
    "                interval = int(interval * 2)\n",
    "\n",
    "        self.next_review_day = current_day + interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "distribution = []\n",
    "file_name = \"Lesson Distribution\"\n",
    "load_existing_distribution = False\n",
    "\n",
    "if load_existing_distribution:\n",
    "    distribution = loadJSON(file_name)\n",
    "\n",
    "else:\n",
    "    base_prompt, response_format = load_prompt_and_format(\n",
    "        \"Prompts/iterative_prompt_cycle.txt\",\n",
    "        \"Prompts/iterative_format_cycle.json\"\n",
    "    )\n",
    "    replacements = {\n",
    "        \"[source_language]\": source_language,\n",
    "        \"[target_language]\": target_language,\n",
    "        \"[CEFR_level]\": CEFR_level,\n",
    "        \"[all_concepts]\": concepts_as_text,\n",
    "    }\n",
    "    \n",
    "    base_prompt = replace_tags(base_prompt, replacements)\n",
    "    \n",
    "    words_so_far = []\n",
    "    concepts_so_far = []\n",
    "    phrases_so_far = []\n",
    "    all_items = []\n",
    "    cycle_number = 1\n",
    "    for cycle in tqdm(topics):\n",
    "        if cycle_number > number_of_topics_to_generate:\n",
    "            break\n",
    "\n",
    "        # Update the prompt to match the current cycle\n",
    "        prompt = base_prompt\n",
    "        replacements = {\n",
    "            # copy dictionary replacements from the commented out lines\n",
    "            \"[cycle_number]\": str(cycle_number),\n",
    "            \"[cycle_topic]\": cycle[\"topic\"],\n",
    "            \"[new_skills]\": \"- \"+\"\\n- \".join(s for s in cycle[\"new_skills\"]),\n",
    "            \"[new_phrases]\": \"- \"+\"\\n- \".join(p for p in cycle[\"new_phrases\"]),\n",
    "            \"[num_previous_cycles]\": str(min(len(distribution),5)),\n",
    "            \"[previous_cycles]\": json.dumps(distribution[-min(len(distribution),5):], indent=2, ensure_ascii=False)\n",
    "        }\n",
    "        prompt = replace_tags(prompt, replacements)\n",
    "\n",
    "        obj = continuously_call_LLM(prompt, system_prompt, response_format)\n",
    "\n",
    "\n",
    "        # Collect the new list of words, concepts, and phrases in this cycle\n",
    "        cycle_words = []\n",
    "        cycle_concepts = []\n",
    "        cycle_phrases = []\n",
    "        lesson_number = (cycle_number-1)*len(obj[\"lessons\"]) + 1\n",
    "        for lesson in obj[\"lessons\"]:\n",
    "            for concept in lesson[\"concepts\"]:\n",
    "                if concept not in concepts_so_far:\n",
    "                    concepts_so_far.append(concept)\n",
    "                    cycle_concepts.append(concept)\n",
    "                    all_items.append(LearningItem(concept, lesson_number, 'concept'))\n",
    "\n",
    "            for word in lesson[\"words\"]:\n",
    "                if word not in words_so_far:\n",
    "                    words_so_far.append(word)\n",
    "                    cycle_words.append(word)\n",
    "                    all_items.append(LearningItem(word, lesson_number, 'word'))\n",
    "\n",
    "            for phrase in lesson[\"phrases\"]:\n",
    "                if phrase not in phrases_so_far:\n",
    "                    phrases_so_far.append(phrase)\n",
    "                    cycle_phrases.append(phrase)\n",
    "                    all_items.append(LearningItem(phrase, lesson_number, 'phrase'))\n",
    "\n",
    "            lesson[\"type\"] = \"normal\"\n",
    "\n",
    "            # Collect the list of items to review for the current lesson based on the spaced repetition algorithm\n",
    "            review_items = [item for item in all_items\n",
    "                            if item.next_review_day and item.next_review_day <= lesson_number]\n",
    "            new_items = [item for item in all_items\n",
    "                            if item.start_lesson == lesson_number and item.exposure_count == 0]\n",
    "\n",
    "            selected_items = (sorted(review_items, key=lambda x: x.next_review_day) +\n",
    "                                new_items)\n",
    "\n",
    "            for item in selected_items:\n",
    "                item.update_after_exposure(lesson_number)\n",
    "\n",
    "            # Include all items to review in the current lesson structure\n",
    "            for item in review_items:\n",
    "                if item.item_type == 'concept':\n",
    "                    if \"conceptsToReview\" in lesson:\n",
    "                        lesson[\"conceptsToReview\"].append(item.content)\n",
    "                    else:\n",
    "                        lesson[\"conceptsToReview\"] = [item.content]\n",
    "                elif item.item_type == 'word':\n",
    "                    if \"wordsToReview\" in lesson:\n",
    "                        lesson[\"wordsToReview\"].append(item.content)\n",
    "                    else:\n",
    "                        lesson[\"wordsToReview\"] = [item.content]\n",
    "                elif item.item_type == 'phrase':\n",
    "                    if \"phrasesToReview\" in lesson:\n",
    "                        lesson[\"phrasesToReview\"].append(item.content)\n",
    "                    else:\n",
    "                        lesson[\"phrasesToReview\"] = [item.content]\n",
    "\n",
    "            lesson_number += 1\n",
    "\n",
    "        # Add the short and long lessons to the current cycle based on the newly added words, concepts, and phrases\n",
    "        obj[\"lessons\"].append({\"type\": \"review\", \"concepts\": cycle_concepts, \"words\": cycle_words, \"phrases\": cycle_phrases})\n",
    "        obj[\"lessons\"].append({\"type\": \"story\", \"concepts\": cycle_concepts, \"words\": cycle_words, \"phrases\": cycle_phrases})\n",
    "\n",
    "        distribution.append(obj)\n",
    "        cycle_number += 1\n",
    "        \n",
    "    saveJSON(distribution, file_name)\n",
    "\n",
    "print(json.dumps(distribution[0], indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1IDBCfw293ZU"
   },
   "source": [
    "# Generate the Lesson Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from docx import Document\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "from docx.enum.text import WD_PARAGRAPH_ALIGNMENT\n",
    "from docx.shared import Pt\n",
    "from docx.oxml import OxmlElement\n",
    "from docx.oxml.ns import qn\n",
    "\n",
    "def contains_rtl(text):\n",
    "    # Check if the text contains any RTL characters\n",
    "    rtl_pattern = re.compile(r'[\\u0600-\\u06FF\\u0750-\\u077F\\u08A0-\\u08FF\\uFB50-\\uFDFF\\uFE70-\\uFEFF]')\n",
    "    return bool(rtl_pattern.search(text))\n",
    "\n",
    "def create_word_document(script, path_to_output):\n",
    "    doc = Document()\n",
    "\n",
    "    # Set the default font\n",
    "    style = doc.styles['Normal']\n",
    "    style.font.name = 'Arial'\n",
    "    style.font.size = Pt(11)\n",
    "    \n",
    "\n",
    "    lines = script.split('\\n')\n",
    "    previous_line_rtl = False\n",
    "    source_is_rtl = contains_rtl(source_language)\n",
    "    target_is_rtl = contains_rtl(target_language)\n",
    "\n",
    "    for line in lines:\n",
    "        # Remove leading/trailing whitespace\n",
    "        line = line.strip()\n",
    "\n",
    "        if line:\n",
    "            # Check if the line starts with a speaker tag\n",
    "            speaker_match = re.match(r'^(\\w+\\))\\s*(.*)', line)\n",
    "\n",
    "            if speaker_match:\n",
    "                speaker, content = speaker_match.groups()\n",
    "                is_rtl = (\"Narrator\" in speaker and source_is_rtl) or (\"Narrator\" not in speaker and target_is_rtl)\n",
    "                \n",
    "                # Convert the parenthesis after the speaker to the correct form for RTL lines\n",
    "                if is_rtl:\n",
    "                    speaker = speaker[:-1]\n",
    "                    content = ') ' + content\n",
    "                else:\n",
    "                    content = ' ' + content\n",
    "\n",
    "                p = doc.add_paragraph(style='Normal')\n",
    "                p.add_run(speaker).bold = True\n",
    "\n",
    "                if content:\n",
    "                    r = p.add_run(content)\n",
    "                    font = r.font\n",
    "                    font.complex_script = True\n",
    "                    font.rtl = is_rtl\n",
    "\n",
    "\n",
    "                p_xml = p._element\n",
    "                bidi = OxmlElement('w:bidi')\n",
    "                bidi.set(qn('w:val'), '1' if is_rtl else '0')\n",
    "                p_xml.get_or_add_pPr().append(bidi)\n",
    "\n",
    "                previous_line_rtl = is_rtl\n",
    "            elif line.strip() == '[pause]':\n",
    "                p = doc.add_paragraph('[pause]', style='Normal')\n",
    "\n",
    "                p.alignment = WD_ALIGN_PARAGRAPH.RIGHT if previous_line_rtl else WD_ALIGN_PARAGRAPH.LEFT\n",
    "            else:\n",
    "                is_rtl = contains_rtl(line) and source_is_rtl\n",
    "\n",
    "                p = doc.add_paragraph(style='Normal')\n",
    "                r = p.add_run(line)\n",
    "                font = r.font\n",
    "                font.complex_script = is_rtl\n",
    "                font.rtl = is_rtl\n",
    "\n",
    "                p_xml = p._element\n",
    "                bidi = OxmlElement('w:bidi')\n",
    "                bidi.set(qn('w:val'), '1' if is_rtl else '0')\n",
    "                p_xml.get_or_add_pPr().append(bidi)\n",
    "\n",
    "                previous_line_rtl = is_rtl\n",
    "    \n",
    "    # Add page numbers to the footer\n",
    "    section = doc.sections[0]\n",
    "    footer = section.footer\n",
    "    footer_paragraph = footer.paragraphs[0]\n",
    "    footer_paragraph.alignment = WD_PARAGRAPH_ALIGNMENT.LEFT\n",
    "\n",
    "    # Add page number field\n",
    "    page_number_run = footer_paragraph.add_run()\n",
    "    fldChar = OxmlElement('w:fldChar')\n",
    "    fldChar.set(qn('w:fldCharType'), 'begin')\n",
    "    page_number_run._element.append(fldChar)\n",
    "\n",
    "    instrText = OxmlElement('w:instrText')\n",
    "    instrText.text = \"PAGE\"\n",
    "    page_number_run._element.append(instrText)\n",
    "\n",
    "    fldChar = OxmlElement('w:fldChar')\n",
    "    fldChar.set(qn('w:fldCharType'), 'end')\n",
    "    page_number_run._element.append(fldChar)\n",
    "\n",
    "    # Save the document\n",
    "    doc.save(path_to_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vX8bgO1A95NW"
   },
   "outputs": [],
   "source": [
    "system_prompt = \"\"\n",
    "with open(\"Prompts/lesson_script_system.txt\", 'r', encoding='utf-8') as file:\n",
    "    system_prompt = file.read()\n",
    "    \n",
    "replacements = {\n",
    "    \"[target_language]\": target_language,\n",
    "    \"[source_language]\": source_language,\n",
    "    \"[CEFR_level]\": CEFR_level,\n",
    "    \"[topic_of_interest]\": topic_of_interest,\n",
    "}\n",
    "\n",
    "system_prompt = replace_tags(system_prompt, replacements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1744712889688,
     "user": {
      "displayName": "Dostee Hemin",
      "userId": "08861143975820381618"
     },
     "user_tz": -180
    },
    "id": "QyG3pizf97dx"
   },
   "outputs": [],
   "source": [
    "base_prompt, response_format = load_prompt_and_format(\n",
    "    \"Prompts/lesson_script_base_prompt.txt\",\n",
    "    \"Prompts/lesson_script_format.json\"\n",
    ")\n",
    "\n",
    "lesson_types = [\"normal\", \"review\", \"story\"]\n",
    "lesson_structures = {}\n",
    "lesson_examples = {}\n",
    "for lesson_type in lesson_types:\n",
    "    with open(f\"Prompts/lesson_script_{lesson_type}_structure.txt\", 'r', encoding='utf-8') as file:\n",
    "        lesson_structures[lesson_type] = file.read()\n",
    "    with open(f\"Prompts/lesson_script_{lesson_type}_example.txt\", 'r', encoding='utf-8') as file:\n",
    "        lesson_examples[lesson_type] = file.read()\n",
    "\n",
    "intro_text = \"\"\n",
    "with open(\"Prompts/intro.txt\", 'r', encoding='utf-8') as file:\n",
    "    intro_text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_number = 1\n",
    "items_so_far = {\"words\":[],\"phrases\":[],\"concepts\":[]}\n",
    "for topic in distribution:\n",
    "  if topic_number > number_of_topics_to_generate:\n",
    "    break\n",
    "\n",
    "  topic_name = topic[\"topic\"]\n",
    "  print(f\"Topic #{topic_number}:  {topic_name}\")\n",
    "  topic_folder = f'Full Script/Topic #{topic_number} - {topic_name}/'\n",
    "  os.makedirs(os.path.dirname(output_folder+topic_folder), exist_ok=True)\n",
    "\n",
    "  lesson_number = 1\n",
    "  for lesson in topic[\"lessons\"]:\n",
    "    # Uncomment this to skip certain lessons or topics if you want to rerun the program\n",
    "    # if topic_number != 2 or lesson_number != 4:\n",
    "    #     lesson_number += 1\n",
    "    #     continue\n",
    "    print(f\"--- Lesson #{lesson_number} ---\")\n",
    "    \n",
    "    # Get the collection of review items and add them in the prompt\n",
    "    review_phrases_text = \"<There are no phrases to review>\"\n",
    "    review_concepts_text = \"<There are no concepts to review>\"\n",
    "    if \"phrasesToReview\" in lesson:\n",
    "      review_phrases_text = \"- \"+\"\\n- \".join(rp for rp in lesson[\"phrasesToReview\"])\n",
    "    if \"conceptsToReview\" in lesson:\n",
    "      review_concepts_text = \"- \"+\"\\n- \".join(rc for rc in lesson[\"conceptsToReview\"])\n",
    "    \n",
    "    # Get the collection of previously learned items and add them in the prompt\n",
    "    old_words_text = \"<There are no previously learned words>\"\n",
    "    old_phrases_text = \"<There are no previously learned phrases>\"\n",
    "    old_concepts_text = \"<There are no previously learned concepts>\"\n",
    "    if len(items_so_far[\"words\"]) > 0:\n",
    "      old_words_text = \"- \"+\"\\n- \".join(ow for ow in items_so_far[\"words\"])\n",
    "    if len(items_so_far[\"phrases\"]) > 0:\n",
    "      old_phrases_text = \"- \"+\"\\n- \".join(op for op in items_so_far[\"phrases\"])\n",
    "    if len(items_so_far[\"concepts\"]) > 0:\n",
    "      old_concepts_text = \"- \"+\"\\n- \".join(oc for oc in items_so_far[\"concepts\"])\n",
    "\n",
    "    # Get the base prompt, add the specific lesson structure, and replace all tags\n",
    "    prompt = base_prompt\n",
    "    \n",
    "    replacements = {\n",
    "        \"[lesson_structure]\": lesson_structures[lesson[\"type\"]],\n",
    "        \"[example]\": lesson_examples[lesson[\"type\"]],\n",
    "        \"[source_language]\": source_language,\n",
    "        \"[target_language]\": target_language,\n",
    "        \"[total_lessons_for_topic]\": str(len(topic[\"lessons\"])),\n",
    "        \"[topic_number]\": str(topic_number),\n",
    "        \"[topic_name]\": topic_name,\n",
    "        \"[lesson_number]\": str(lesson_number),\n",
    "        \"[new_words]\": \"- \"+\"\\n- \".join(w for w in lesson[\"words\"]),\n",
    "        \"[new_phrases]\": \"- \"+\"\\n- \".join(p for p in lesson[\"phrases\"]),\n",
    "        \"[new_concepts]\": \"- \"+\"\\n- \".join(c for c in lesson[\"concepts\"]),\n",
    "        \"[next_lesson]\": \"\" if lesson_number == 5 else json.dumps(topic['lessons'][lesson_number], indent=2, ensure_ascii=False),\n",
    "        \"[next_topic]\": distribution[topic_number][\"topic\"] if topic_number < len(distribution) else \"<There are no more topics>\",\n",
    "        \"[review_phrases]\": review_phrases_text,\n",
    "        \"[review_concepts]\": review_concepts_text,\n",
    "        \"[old_words]\": old_words_text,\n",
    "        \"[old_phrases]\": old_phrases_text,\n",
    "        \"[old_concepts]\": old_concepts_text,\n",
    "      \n",
    "    }\n",
    "    \n",
    "    prompt = replace_tags(prompt, replacements)\n",
    "\n",
    "    if \"العامية\" in source_language or \"العامية\" in target_language:\n",
    "      prompt += f'\\nSince the one of the languages you need to consider uses colloquial Arabic, you must speak using colloquial Arabic in the dialect specified. You can not use standard Arabic. Even if you don\\'t know the dialect fully, try your absolute best to speak in the colloquial Arabic of the specified dialect.'\n",
    "\n",
    "    if lesson_number == 1:\n",
    "      prompt += f'''\n",
    "      - Preface this lesson with the narrator saying the following intro:\n",
    "      <intro>\n",
    "      {intro_text}\n",
    "      </intro>\n",
    "      \n",
    "      Note that this intro must be translated into {source_language} without changing the meaning or altering the structure. Keep in mind that \"Agora Vision\" must be kept as is, not translated or transliterated. Transition into the lesson script seemlessly.\n",
    "      '''\n",
    "\n",
    "    script_structure = continuously_call_LLM(prompt, system_prompt, response_format, models['gemini 2.5 preview'])\n",
    "    script = '\\n'.join(section['text'] for section in script_structure)\n",
    "\n",
    "    # Save the lesson script in its own text file\n",
    "    path_to_output = output_folder + topic_folder + f\"Lesson {lesson_number}.docx\"\n",
    "    create_word_document(script, path_to_output)\n",
    "\n",
    "    # Add the unique new items learned in this lesson to the total items encountered so far\n",
    "    for word in lesson[\"words\"]:\n",
    "      if word not in items_so_far[\"words\"]:\n",
    "        items_so_far[\"words\"].append(word)\n",
    "    for phrase in lesson[\"phrases\"]:\n",
    "      if phrase not in items_so_far[\"phrases\"]:\n",
    "        items_so_far[\"phrases\"].append(phrase)\n",
    "    for concept in lesson[\"concepts\"]:\n",
    "      if concept not in items_so_far[\"concepts\"]:\n",
    "        items_so_far[\"concepts\"].append(concept)\n",
    "    lesson_number += 1\n",
    "\n",
    "  topic_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNA1NoHfJDkq0b8kiIrO3Wx",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
